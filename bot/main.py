import asyncio
import os
import re
from datetime import datetime, timezone
from typing import Iterable
from urllib.parse import quote, unquote

from openai import OpenAI
from telethon import TelegramClient
from telethon.sessions import StringSession
from telethon.tl.custom.message import Message

# ============
# ENV / SECRETS
# ============

def _must_get_env(name: str) -> str:
    value = os.getenv(name)
    if value is None or value == "":
        raise RuntimeError(f"Missing required environment variable: {name}")
    return value


def _get_bool_env(name: str, default: bool = False) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default

    normalized = raw.strip().lower()
    return normalized in {"1", "true", "yes", "y", "on"}


def _get_list_env(name: str) -> list[str]:
    raw = os.getenv(name)
    if raw is None or not raw.strip():
        return []
    return [value.strip().lower() for value in raw.split(",") if value.strip()]


tg_api_id = int(_must_get_env("TG_API_ID"))
tg_api_hash = _must_get_env("TG_API_HASH")
tg_session = _must_get_env("TG_SESSION")

tg_source_channels = [
    c.strip() for c in _must_get_env("TG_SOURCE_CHANNELS").split(",") if c.strip()
]

if not tg_source_channels:
    raise RuntimeError("TG_SOURCE_CHANNELS is set but empty after parsing")

tg_target_channel = _must_get_env("TG_TARGET_CHANNEL")
affiliate_portal_template = (os.getenv("AFFILIATE_PORTAL_LINK") or "").strip()
affiliate_prefix = (os.getenv("AFFILIATE_PREFIX") or "").strip()
openai_api_key = _must_get_env("OPENAI_API_KEY")

openai_model = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
min_views = int(os.getenv("MIN_VIEWS", "1500"))
max_messages_per_channel = int(os.getenv("MAX_MESSAGES_PER_CHANNEL", "80"))
dry_run = _get_bool_env("DRY_RUN", False)
max_posts_per_run = int(os.getenv("MAX_POSTS_PER_RUN", "5"))
message_cooldown_seconds = int(os.getenv("MESSAGE_COOLDOWN_SECONDS", "5"))
max_message_age_minutes = int(os.getenv("MAX_MESSAGE_AGE_MINUTES", "240"))
keyword_allowlist = _get_list_env("KEYWORD_ALLOWLIST")
keyword_blocklist = _get_list_env("KEYWORD_BLOCKLIST")

client = TelegramClient(StringSession(tg_session), tg_api_id, tg_api_hash)
oa_client = OpenAI(api_key=openai_api_key)
processed_product_ids: set[str] = set()

if not affiliate_portal_template and not affiliate_prefix:
    raise RuntimeError(
        "You must provide either AFFILIATE_PORTAL_LINK (preferred) or AFFILIATE_PREFIX"
    )

# ============
# UTILITIES
# ============

ali_regex = re.compile(r"https?://[^\s]*aliexpress\.com[^\s]*", re.IGNORECASE)
url_regex = re.compile(r"https?://\S+")


def _canonical_url(url: str) -> str:
    """Normalize URLs for comparison/removal (trim brackets/punctuation)."""

    cleaned = url.strip().strip("[]()<>.,")
    return cleaned


def extract_aliexpress_links(text: str) -> list[str]:
    if not text:
        return []
    return ali_regex.findall(text)


def normalize_aliexpress_id(url: str) -> str:
    """Extract a stable identifier to detect duplicates."""
    normalized_url = _canonical_url(url)

    click_match = re.search(
        r"s\.click\.aliexpress\.com/(?:e|aw)/(_?[A-Za-z0-9]+)", normalized_url,
        re.IGNORECASE,
    )
    if click_match:
        return click_match.group(1).lstrip("_")

    match = re.search(r"/item/(\d+)\.html", normalized_url)
    if match:
        return match.group(1)

    match = re.search(r"/(\d+)\.html", normalized_url)
    if match:
        return match.group(1)

    return normalized_url.split("?")[0]


def make_affiliate_link(original_url: str) -> str:
    cleaned = _canonical_url(unquote(original_url))
    encoded = quote(cleaned, safe="")

    if affiliate_portal_template:
        if "{url}" in affiliate_portal_template:
            return affiliate_portal_template.replace("{url}", encoded).strip()

        # If the portal link is already a full tracking link, use it verbatim.
        return affiliate_portal_template.strip()

    return f"{affiliate_prefix}{encoded}"


def strip_non_affiliate_links(content: str, affiliate_url: str) -> str:
    """Remove original/extra URLs so only the personal affiliate link remains."""

    normalized_aff_url = _canonical_url(affiliate_url)
    seen_affiliate = False

    def _replacer(match: re.Match[str]) -> str:
        nonlocal seen_affiliate
        url = _canonical_url(match.group(0))

        if normalized_aff_url in url and not seen_affiliate:
            seen_affiliate = True
            return normalized_aff_url

        return ""

    cleaned = url_regex.sub(_replacer, content)
    occurrences = cleaned.count(normalized_aff_url)
    if occurrences > 1:
        cleaned = cleaned.replace(normalized_aff_url, "", occurrences - 1)
    return cleaned.strip()


def ensure_affiliate_link(content: str, affiliate_url: str) -> tuple[str, bool]:
    """Guarantee the personal affiliate link appears exactly once.

    Returns the possibly-updated content and whether an append was needed.
    """
    normalized_aff_url = _canonical_url(affiliate_url)

    if normalized_aff_url in content:
        return content, False

    enforced_block = f"ğŸ‘‡ ×œ×§× ×™×™×” ×‘××œ×™××§×¡×¤×¨×¡:\n{normalized_aff_url}"

    # If the prompt added the header without the link, attach it to the same block.
    if "ğŸ‘‡ ×œ×§× ×™×™×” ×‘××œ×™××§×¡×¤×¨×¡" in content:
        return content.rstrip() + f"\n{normalized_aff_url}", True

    return content.rstrip() + f"\n\n{enforced_block}", True


def _fallback_caption(orig_text: str, affiliate_url: str) -> str:
    """Create a minimal, deterministic caption if the model yields nothing."""
    cleaned = orig_text.strip().splitlines()
    headline = cleaned[0] if cleaned else "××¦××ª×™ ×“×™×œ ×©×©×•×•×” ×œ×”×¦×™×¥ ×‘×•"
    bullets = [line for line in cleaned[1:6] if line.strip()][:4]
    bullet_block = (
        "\n".join(f"â€¢ {b.strip()}" for b in bullets)
        if bullets
        else "â€¢ ×œ×¤×¨×˜×™× × ×•×¡×¤×™× ×‘×§×™×©×•×¨"
    )

    base = "\n".join(
        [
            headline,
            "×”× ×” ×”×¤×¨×˜×™× ×‘×§×¦×¨×”:",
            bullet_block,
            "ğŸ‘‡ ×œ×§× ×™×™×” ×‘××œ×™××§×¡×¤×¨×¡:",
            _canonical_url(affiliate_url),
        ]
    )
    return base.strip()


def enforce_single_affiliate_link(content: str, affiliate_url: str) -> str:
    """Strip all HTTP URLs except the one affiliate link (kept once)."""

    normalized_aff_url = _canonical_url(affiliate_url)
    seen_affiliate = False

    def _replacer(match: re.Match[str]) -> str:
        nonlocal seen_affiliate
        url = _canonical_url(match.group(0))

        if normalized_aff_url in url and not seen_affiliate:
            seen_affiliate = True
            return normalized_aff_url

        return ""

    cleaned = url_regex.sub(_replacer, content)

    if not seen_affiliate:
        enforced_block = f"\n\nğŸ‘‡ ×œ×§× ×™×™×” ×‘××œ×™××§×¡×¤×¨×¡:\n{normalized_aff_url}"
        cleaned = f"{cleaned.strip()}{enforced_block}".strip()

    occurrences = cleaned.count(normalized_aff_url)
    if occurrences > 1:
        cleaned = cleaned.replace(normalized_aff_url, "", occurrences - 1)

    return cleaned.strip()


def extract_fact_hints(text: str) -> dict[str, str]:
    """Pull simple structured hints (price/rating/orders/coupons) from the source."""

    hints: dict[str, str] = {}

    price_match = re.search(r"(â‚ª|\$)\s?\d+[\d.,]*", text)
    if price_match:
        hints["price"] = price_match.group(0)

    rating_match = re.search(r"(?:â­|rating[:\s]*)(\d+(?:\.\d+)?)", text, re.IGNORECASE)
    if rating_match:
        hints["rating"] = rating_match.group(1)

    orders_match = re.search(r"(\d[\d.,]*\+?)\s*(?:orders|×”×–×× ×•×ª|sold)", text, re.IGNORECASE)
    if orders_match:
        hints["orders"] = orders_match.group(1)

    coupon_matches = re.findall(r"(?:×§×•×¤×•×Ÿ|coupon|code)[:\s]*([A-Za-z0-9-]+)", text, re.IGNORECASE)
    if coupon_matches:
        hints["coupons"] = ", ".join(dict.fromkeys(coupon_matches))

    return hints


def rewrite_caption(orig_text: str, affiliate_url: str) -> str:
    hints = extract_fact_hints(orig_text)
    if hints:
        hints_lines = [
            "× ×ª×•× ×™× ×©×–×•×”×• ×‘×˜×§×¡×˜:",
            *(f"- {key}: {value}" for key, value in hints.items()),
        ]
        hints_block = "\n".join(hints_lines)
    else:
        hints_block = "×œ× × ××¦××• × ×ª×•× ×™ ××—×™×¨/×“×™×¨×•×’/×”×–×× ×•×ª/×§×•×¤×•× ×™× ×‘×˜×§×¡×˜."

    prompt = f"""
××ª×” ×›×•×ª×‘ ×¤×•×¡×˜ ×“×™×œ ×œ×§×‘×•×¦×ª ×“×™×œ×™× ×™×©×¨××œ×™×ª (×•×•××˜×¡××¤ / ×˜×œ×’×¨×).
×”××˜×¨×”: ×¤×•×¡×˜ ××•×›×Ÿ ××—×“-×œ××—×“ ×œ×”×¢×ª×§×”.

×—×•×§×™ ×¡×’× ×•×Ÿ:
- ×›×ª×™×‘×” ×¨×§ ×‘×¢×‘×¨×™×ª, ×˜×•×Ÿ ×™×•××™×•××™, ×™×©×¨××œ×™, ×§×¦×¨, ×¢× ×˜×™×¤×” ×™×•×ª×¨ ×—×™×™× ×•×”×•××•×¨ ×¢×“×™×Ÿ.
- 1â€“3 ××™××•×’'×™× ×‘×¡×š ×”×›×•×œ (×œ× ×™×•×ª×¨) ×©××¨×’×™×©×™× ×˜×‘×¢×™×™× ×œ××•×¦×¨.
- ××©×¤×˜×™× ×§×¦×¨×™×, ×‘×œ×™ ×”×¤×¨×–×•×ª ×•×‘×œ×™ "×”×“×™×œ ×”×›×™ ××˜×•×¨×£ ×‘×¢×•×œ×".
- ×œ× ×œ×”××¦×™× ××™×“×¢ ×©×œ× ×§×™×™× ×‘××§×•×¨.
- ×× × ×ª×•×Ÿ (××—×™×¨/×“×™×¨×•×’/×”×–×× ×•×ª/×§×•×¤×•× ×™×) ×œ× ××•×¤×™×¢ ×‘××™×“×¢ â€“ ××“×œ×’×™× ×¢×œ×™×•.

××‘× ×” ××—×™×™×‘ ×©×œ ×”×¤×•×¡×˜ (×ª××™×“ ×œ×©××•×¨ ×¢×œ×™×•):
1) ×©×•×¨×ª ×¤×ª×™×—×” â€“ ×©××œ×” ×™×•××™×•××™×ª ×©××ª××™××” ×œ××•×¦×¨ (×©×•×¨×” ××—×ª).
2) ××©×¤×˜ ××—×“ ×§×¦×¨ ×©××¦×™×’ ××ª ×”××•×¦×¨ ×›×¤×ª×¨×•×Ÿ ×‘×¨×•×¨ ×œ×©××œ×”.
3) ×‘×•×œ×˜×™× ×ª×›×œ'×¡ â€“ 3â€“6 × ×§×•×“×•×ª, ×§×¦×¨×•×ª (×¢×“ ~7â€“9 ××™×œ×™×):
   - ×¡×•×’/×“×’×/×©×™××•×©×™×/×™×ª×¨×•× ×•×ª/×¤×¨×˜×™× ×˜×›× ×™×™× ×—×©×•×‘×™×.
4) × ×ª×•× ×™ ××—×™×¨/×“×™×¨×•×’/×”×–×× ×•×ª â€“ ×¨×§ ×× ×§×™×™××™× ×‘××™×“×¢:
   - "ğŸ’° ××—×™×¨ ××—×¨×™ ×”× ×—×•×ª: <××—×™×¨>" (××¤×©×¨ ×’× ××—×™×¨ $ ×‘×¡×•×’×¨×™×™× ×× ×”×•×¤×™×¢).
   - "â­ ×“×™×¨×•×’: X.X" ×× ×™×©.
   - "ğŸ“¦ ××¡' ×”×–×× ×•×ª: XXXX+" ×× ×™×©.
   - ×× ×›×ª×•×‘ ×©×”××™×¡×™× ×œ× ×›×œ×•×œ×™× â€“ ×œ×¦×™×™×Ÿ ×‘××©×¤×˜ ×§×¦×¨.
5) ×§×•×¤×•× ×™× â€“ ×¨×§ ×× ×§×™×™××™× ×‘××™×“×¢:
   - ×©×•×¨×”: "ğŸ ×§×•×¤×•× ×™×:" ×•××– ×¨×©×™××” ××¡×•×“×¨×ª; ×× ×¦×¨×™×š ×¡×“×¨ ×©×™××•×© â€“ ×œ×¦×™×™×Ÿ "×§×•×“× X ×•××– Y".
6) ×§×™×©×•×¨ ×§× ×™×™×”:
   - ×©×•×¨×”: "ğŸ‘‡ ×œ×§× ×™×™×” ×‘××œ×™××§×¡×¤×¨×¡:"
   - ×‘×©×•×¨×” ×”×‘××”: ×”×œ×™× ×§ {affiliate_url}

×“×’×©×™×:
- ×œ×”×©×ª××© ×¨×§ ×‘××™×“×¢ ×©××•×¤×™×¢ ×‘×˜×§×¡×˜ ×”××§×•×¨×™ ×©×œ ×”×¤×•×¡×˜ (××• ×‘×œ×™× ×§ ×× ××•×–×›×¨). ×œ× ×œ×”××¦×™×.
- ××œ ×ª×–×›×™×¨ ×©××ª×” ××¢×ª×™×§ ××• ××§×‘×•×¦×” ××—×¨×ª. ×œ× ×œ×¦×™×™×Ÿ "××œ×™××§×¡×¤×¨×¡" ×¤×¨×˜ ×œ×©×•×¨×ª ×”×§× ×™×™×”.
- ×”×•××•×¨ ×¢×“×™×Ÿ ×•×§×¦×¨, ×‘×œ×™ ×¦×¢×§×•×ª.
- ××œ ×ª×•×¡×™×£ ×§×™×©×•×¨×™× ××—×¨×™× ××œ×‘×“ ×”×œ×™× ×§ ×©×¡×™×¤×§×ª×™ ×‘×©×•×¨×ª ×”×§× ×™×™×”, ×•×œ× ×œ×—×–×•×¨ ×¢×œ×™×• ×¤×¢××™×™×.

{hints_block}

×”× ×” ×”××™×“×¢ ×”×’×•×œ××™ ×©×¢×œ×™×• ××¡×ª××š ×”×¤×•×¡×˜ (×ª×™××•×¨/××—×™×¨/×“×™×¨×•×’/×§×•×¤×•× ×™×/×§×™×©×•×¨ ×•×›×•'):
---
{orig_text}
---
"""

    response = oa_client.chat.completions.create(
        model=openai_model,
        messages=[
            {
                "role": "system",
                "content": (
                    "××ª×” ×›×•×ª×‘ ×§×•×¤×™ ×‘×¢×‘×¨×™×ª ×œ×§×‘×•×¦×ª ×“×™×œ×™× ×‘×˜×œ×’×¨×. ×©××•×¨ ×¢×œ ××‘× ×” ×§×‘×•×¢, ×œ× "
                    "×××¦×™× ×¤×¨×˜×™×, ×•××©×ª××© ×‘-1â€“3 ××™××•×’'×™× ×‘×œ×‘×“."
                ),
            },
            {"role": "user", "content": prompt.strip()},
        ],
        temperature=0.6,
        max_tokens=500,
    )
    content = response.choices[0].message.content.strip()
    if not content:
        log_info("OpenAI returned empty content; using fallback caption")
        return _fallback_caption(orig_text, affiliate_url)

    return content


def evaluate_post_quality(msg: Message) -> tuple[bool, str | None]:
    if not msg.message:
        return False, "empty message"

    text = msg.message.lower()
    keywords = ["â‚ª", "$", "discount", "coupon", "×§×•×¤×•×Ÿ", "×“×™×œ", "××‘×¦×¢", "%", "×§×•×“"]

    if keyword_blocklist and any(blocked in text for blocked in keyword_blocklist):
        return False, "blocked keyword"

    allow_sources: Iterable[str] = keyword_allowlist if keyword_allowlist else keywords
    if not any(keyword in text for keyword in allow_sources):
        return False, "missing keywords"

    if msg.views is not None and msg.views < min_views:
        return False, "below min views"

    if msg.date:
        message_dt = msg.date
        if message_dt.tzinfo is None:
            message_dt = message_dt.replace(tzinfo=timezone.utc)

        age_minutes = (datetime.now(timezone.utc) - message_dt).total_seconds() / 60
        if age_minutes > max_message_age_minutes:
            return False, "too old"

    return True, None


def format_message(content: str, product_id: str) -> str:
    return f"{content}\n\n(id:{product_id})"


def log_info(message: str) -> None:
    print(message, flush=True)


def describe_affiliate_mode() -> str:
    if affiliate_portal_template:
        if "{url}" in affiliate_portal_template:
            return "portal template with {url} placeholder"
        return "portal template (verbatim link)"

    return "prefix-based affiliate link"


async def already_posted(product_id: str) -> bool:
    async for msg in client.iter_messages(tg_target_channel, limit=300):
        if not isinstance(msg, Message) or not msg.message:
            continue
        if f"(id:{product_id})" in msg.message:
            return True
    return False


# ============
# MAIN FLOW
# ============


async def process_channel(channel: str) -> tuple[int, dict[str, int]]:
    log_info(f"Scanning source channel: {channel}")

    posted_count = 0
    skip_reasons: dict[str, int] = {}

    def _mark_skip(reason: str) -> None:
        skip_reasons[reason] = skip_reasons.get(reason, 0) + 1

    async for msg in client.iter_messages(channel, limit=max_messages_per_channel):
        if not isinstance(msg, Message) or not msg.message:
            continue

        links = extract_aliexpress_links(msg.message)
        if not links:
            _mark_skip("no aliexpress link")
            continue

        is_good, reason = evaluate_post_quality(msg)
        if not is_good:
            _mark_skip(reason or "unknown")
            log_info(f"Skip message in {channel}: {reason}")
            continue

        original_url = links[0]
        product_id = normalize_aliexpress_id(original_url)

        if product_id in processed_product_ids:
            log_info(f"Already handled product_id={product_id} earlier this run; skipping")
            _mark_skip("duplicate this run")
            continue

        if await already_posted(product_id):
            log_info(f"Already posted product_id={product_id}, skipping")
            processed_product_ids.add(product_id)
            _mark_skip("duplicate previously posted")
            continue

        affiliate_url = make_affiliate_link(original_url)

        source_without_links = strip_non_affiliate_links(msg.message, affiliate_url)
        if source_without_links != msg.message:
            log_info("Stripped original links from source message to enforce personal URL")

        try:
            new_caption = rewrite_caption(source_without_links or msg.message, affiliate_url)
        except Exception as exc:  # noqa: BLE001
            log_info(f"OpenAI rewrite error: {exc}")
            _mark_skip("rewrite error fallback")
            new_caption = f"{source_without_links or msg.message}\n\nğŸ”— ×œ×™× ×§: {affiliate_url}"

        cleaned_caption = strip_non_affiliate_links(new_caption, affiliate_url)
        secured_caption, appended_link = ensure_affiliate_link(cleaned_caption, affiliate_url)
        if appended_link:
            log_info(
                "Affiliate link was missing from the rewritten text; appended personal link"
            )

        final_caption = enforce_single_affiliate_link(secured_caption, affiliate_url)
        if final_caption != secured_caption:
            log_info("Cleaned extra URLs to keep only the personal affiliate link once")

        final_text = format_message(final_caption, product_id)

        if dry_run:
            log_info(
                "DRY_RUN is enabled; skipping send. Would have posted "
                f"product_id={product_id} to {tg_target_channel}"
            )
            posted_count += 1
            processed_product_ids.add(product_id)
            if posted_count >= max_posts_per_run:
                log_info(
                    "Reached MAX_POSTS_PER_RUN in DRY_RUN mode; stopping further processing"
                )
                break
            continue

        try:
            await client.send_message(tg_target_channel, final_text)
            log_info(f"Posted product_id={product_id} to {tg_target_channel}")
        except Exception as exc:  # noqa: BLE001
            log_info(f"Error sending message to target channel: {exc}")

        posted_count += 1
        processed_product_ids.add(product_id)
        if posted_count >= max_posts_per_run:
            log_info("Reached MAX_POSTS_PER_RUN; stopping further processing")
            break

        if message_cooldown_seconds > 0:
            await asyncio.sleep(message_cooldown_seconds)

    if skip_reasons:
        details = ", ".join(f"{reason}: {count}" for reason, count in skip_reasons.items())
        log_info(f"Channel {channel} skip summary -> {details}")

    return posted_count, skip_reasons


async def main() -> None:
    log_info(
        "Starting run with "
        f"dry_run={dry_run}, sources={len(tg_source_channels)}, "
        f"target={tg_target_channel}, affiliate_mode={describe_affiliate_mode()}, "
        f"max_posts_per_run={max_posts_per_run}"
    )

    total_posted = 0
    total_skip_reasons: dict[str, int] = {}

    for channel in tg_source_channels:
        channel_posted, channel_skips = await process_channel(channel)
        total_posted += channel_posted

        for reason, count in channel_skips.items():
            total_skip_reasons[reason] = total_skip_reasons.get(reason, 0) + count

    if total_skip_reasons:
        summary = ", ".join(
            f"{reason}: {count}" for reason, count in total_skip_reasons.items()
        )
        log_info(f"Overall skip summary -> {summary}")

    log_info(
        "Run completed. Posts sent (including DRY_RUN counts): "
        f"{total_posted}"
    )


if __name__ == "__main__":
    asyncio.run(client.start())
    with client:
        client.loop.run_until_complete(main())
